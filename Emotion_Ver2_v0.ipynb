{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  folder data path\n",
    "\n",
    "path_train = './dataHappy/train/'\n",
    "path_validation = './dataHappy/test/'\n",
    "\n",
    "path_train_c1 = './dataHappy/train/normal/'\n",
    "path_train_c2 = './dataHappy/train/small_smile/'\n",
    "path_train_c3 = './dataHappy/train/smile/'\n",
    "path_train_c4 = './dataHappy/train/big_smile/'\n",
    "\n",
    "path_val_c1 = './dataHappy/test/normal/'\n",
    "path_val_c2 = './dataHappy/test/small_smile/'\n",
    "path_val_c3 = './dataHappy/test/smile/'\n",
    "path_val_c4 = './dataHappy/test/big_smile/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train images train_c1, train_c2, train_c3, train_c4:  3591 3741 3026 4101\n",
      "Total validation images val_c1, val_c2, val_c3, val_c4:  897 654 756 1026\n"
     ]
    }
   ],
   "source": [
    "# Check the number of pictures in train and validation\n",
    "train_c1 = os.listdir(path_train_c1)\n",
    "train_c2 = os.listdir(path_train_c2)\n",
    "train_c3 = os.listdir(path_train_c3)\n",
    "train_c4 = os.listdir(path_train_c4)\n",
    "\n",
    "val_c1 = os.listdir(path_val_c1)\n",
    "val_c2 = os.listdir(path_val_c2)\n",
    "val_c3 = os.listdir(path_val_c3)\n",
    "val_c4 = os.listdir(path_val_c4)\n",
    "\n",
    "print('Total train images train_c1, train_c2, train_c3, train_c4: ', len(train_c1), len(train_c2), len(train_c3), len(train_c4))\n",
    "print('Total validation images val_c1, val_c2, val_c3, val_c4: ', len(val_c1), len(val_c2), len(val_c3), len(val_c4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal', 'small_smile', 'smile', 'big_smile']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rotate and flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def count_images_in_directory(directory):\n",
    "    image_count = 0\n",
    "    for name in os.listdir(directory):\n",
    "        if name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "def sinh_data(path):\n",
    "    max_image_count = 0\n",
    "    max_image_directory = None\n",
    "\n",
    "    # Find the folder has the most pictures\n",
    "    for sub_directory in os.listdir(path):\n",
    "        sub_directory_path = os.path.join(path, sub_directory)\n",
    "\n",
    "        if os.path.isdir(sub_directory_path):\n",
    "            image_count = count_images_in_directory(sub_directory_path)\n",
    "            if image_count > max_image_count:\n",
    "                max_image_count = image_count\n",
    "                max_image_directory = sub_directory\n",
    "\n",
    "    # Deal with sub_folders that have fewer images than the largest folder\n",
    "    for sub_directory in os.listdir(path):\n",
    "        sub_directory_path = os.path.join(path, sub_directory)\n",
    "\n",
    "        if os.path.isdir(sub_directory_path) and sub_directory != max_image_directory:\n",
    "            image_count = count_images_in_directory(sub_directory_path)\n",
    "\n",
    "            if image_count <= 1.5 * max_image_count:\n",
    "                for name in os.listdir(sub_directory_path):\n",
    "                    if name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        image = cv2.imread(os.path.join(sub_directory_path, name))\n",
    "                        height, width = image.shape[:2]\n",
    "                        angle = 20\n",
    "                        rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), angle, 1)\n",
    "                        rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "                        flipped_image = cv2.flip(rotated_image, 1)\n",
    "                        output_directory = os.path.join(path, sub_directory)\n",
    "                        os.makedirs(output_directory, exist_ok=True)\n",
    "                        output_path = os.path.join(output_directory, f'sinh_{name}')\n",
    "                        cv2.imwrite(output_path, flipped_image)\n",
    "\n",
    "\n",
    "dir = [path_train, path_validation]\n",
    "for i in dir:\n",
    "    sinh = sinh_data(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19929 images belonging to 4 classes.\n",
      "Found 4606 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(path_train, \n",
    "                                                    # batch_size=16, \n",
    "                                                    class_mode='categorical', \n",
    "                                                    target_size=(48, 48), \n",
    "                                                    color_mode='grayscale')\n",
    "validation_generator = datagen.flow_from_directory(path_validation, \n",
    "                                                            #   batch_size=16, \n",
    "                                                              class_mode='categorical', \n",
    "                                                              target_size=(48, 48), \n",
    "                                                              color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Re-create callback to save the best weights\n",
    "checkpoint = ModelCheckpoint(\"./src/best_model.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.2752\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32827, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 476s 761ms/step - loss: 1.5299 - accuracy: 0.2752 - val_loss: 1.3889 - val_accuracy: 0.3283\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.3923 - accuracy: 0.3009\n",
      "Epoch 2: val_accuracy improved from 0.32827 to 0.33283, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 441s 707ms/step - loss: 1.3923 - accuracy: 0.3009 - val_loss: 1.3705 - val_accuracy: 0.3328\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.3763 - accuracy: 0.3148\n",
      "Epoch 3: val_accuracy improved from 0.33283 to 0.37060, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 427s 686ms/step - loss: 1.3763 - accuracy: 0.3148 - val_loss: 1.3507 - val_accuracy: 0.3706\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.2969 - accuracy: 0.3724\n",
      "Epoch 4: val_accuracy improved from 0.37060 to 0.38406, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 436s 700ms/step - loss: 1.2969 - accuracy: 0.3724 - val_loss: 1.2015 - val_accuracy: 0.3841\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.1130 - accuracy: 0.4645\n",
      "Epoch 5: val_accuracy improved from 0.38406 to 0.45376, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 439s 704ms/step - loss: 1.1130 - accuracy: 0.4645 - val_loss: 1.1207 - val_accuracy: 0.4538\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.5389\n",
      "Epoch 6: val_accuracy did not improve from 0.45376\n",
      "623/623 [==============================] - 447s 717ms/step - loss: 1.0003 - accuracy: 0.5389 - val_loss: 1.8092 - val_accuracy: 0.3359\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.9068 - accuracy: 0.5827\n",
      "Epoch 7: val_accuracy improved from 0.45376 to 0.46570, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 461s 740ms/step - loss: 0.9068 - accuracy: 0.5827 - val_loss: 1.2132 - val_accuracy: 0.4657\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.8697 - accuracy: 0.6049\n",
      "Epoch 8: val_accuracy improved from 0.46570 to 0.57490, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 438s 703ms/step - loss: 0.8697 - accuracy: 0.6049 - val_loss: 0.8754 - val_accuracy: 0.5749\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.8127 - accuracy: 0.6292\n",
      "Epoch 9: val_accuracy did not improve from 0.57490\n",
      "623/623 [==============================] - 456s 731ms/step - loss: 0.8127 - accuracy: 0.6292 - val_loss: 1.0512 - val_accuracy: 0.5321\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.6747\n",
      "Epoch 10: val_accuracy improved from 0.57490 to 0.60899, saving model to ./src\\best_model.hdf5\n",
      "623/623 [==============================] - 467s 749ms/step - loss: 0.7528 - accuracy: 0.6747 - val_loss: 0.9016 - val_accuracy: 0.6090\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        batch_size=16, \n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"./src/best_model.json\",'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
